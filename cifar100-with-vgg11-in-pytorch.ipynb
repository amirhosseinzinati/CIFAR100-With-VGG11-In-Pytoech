{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545a614f",
   "metadata": {
    "papermill": {
     "duration": 0.003446,
     "end_time": "2023-10-13T12:01:53.598156",
     "exception": false,
     "start_time": "2023-10-13T12:01:53.594710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cifar100 with VGG11 In Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584bd476",
   "metadata": {
    "papermill": {
     "duration": 0.002481,
     "end_time": "2023-10-13T12:01:53.603506",
     "exception": false,
     "start_time": "2023-10-13T12:01:53.601025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429048a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T12:01:53.611164Z",
     "iopub.status.busy": "2023-10-13T12:01:53.610485Z",
     "iopub.status.idle": "2023-10-13T12:02:00.136058Z",
     "shell.execute_reply": "2023-10-13T12:02:00.135098Z"
    },
    "papermill": {
     "duration": 6.532064,
     "end_time": "2023-10-13T12:02:00.138254",
     "exception": false,
     "start_time": "2023-10-13T12:01:53.606190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df85a656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T12:02:00.145241Z",
     "iopub.status.busy": "2023-10-13T12:02:00.144785Z",
     "iopub.status.idle": "2023-10-13T12:02:00.245956Z",
     "shell.execute_reply": "2023-10-13T12:02:00.244906Z"
    },
    "papermill": {
     "duration": 0.106521,
     "end_time": "2023-10-13T12:02:00.247776",
     "exception": false,
     "start_time": "2023-10-13T12:02:00.141255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ngpu = 2                    # Number of GPUs available. Use 0 for CPU mode.\n",
    "# Device configuration\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b649292",
   "metadata": {
    "papermill": {
     "duration": 0.002683,
     "end_time": "2023-10-13T12:02:00.253525",
     "exception": false,
     "start_time": "2023-10-13T12:02:00.250842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb59e62",
   "metadata": {
    "papermill": {
     "duration": 0.002406,
     "end_time": "2023-10-13T12:02:00.258691",
     "exception": false,
     "start_time": "2023-10-13T12:02:00.256285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CIFAR100 Dataset\n",
    "For this Project, we'll use the CIFAR100 dataset. This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses.\n",
    "\n",
    "![Cifar100](https://datasets.activeloop.ai/wp-content/uploads/2022/09/CIFAR-100-dataset-Activeloop-Platform-visualization-image.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4a04c",
   "metadata": {
    "papermill": {
     "duration": 0.002381,
     "end_time": "2023-10-13T12:02:00.263852",
     "exception": false,
     "start_time": "2023-10-13T12:02:00.261471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f00e094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T12:02:00.270524Z",
     "iopub.status.busy": "2023-10-13T12:02:00.270268Z",
     "iopub.status.idle": "2023-10-13T12:02:09.438137Z",
     "shell.execute_reply": "2023-10-13T12:02:09.437191Z"
    },
    "papermill": {
     "duration": 9.173767,
     "end_time": "2023-10-13T12:02:09.440231",
     "exception": false,
     "start_time": "2023-10-13T12:02:00.266464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:02<00:00, 78508557.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=True,\n",
    "                test=False):\n",
    "  \n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR100(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR100( root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    valid_dataset = datasets.CIFAR100(root=data_dir, train=True,download=True, transform=transform,)\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    " \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "# flower 102 dataset \n",
    "train_loader, valid_loader = data_loader(data_dir='./data',\n",
    "                                         batch_size=64)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data',\n",
    "                              batch_size=64,\n",
    "                              test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0b31d",
   "metadata": {
    "papermill": {
     "duration": 0.003922,
     "end_time": "2023-10-13T12:02:09.448455",
     "exception": false,
     "start_time": "2023-10-13T12:02:09.444533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## VGG11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79be809",
   "metadata": {
    "papermill": {
     "duration": 0.003613,
     "end_time": "2023-10-13T12:02:09.456183",
     "exception": false,
     "start_time": "2023-10-13T12:02:09.452570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the [paper](https://arxiv.org/pdf/1409.1556v6.pdf), the authors introduced not one but six different network configurations for the VGG neural network models. Each of them has a different neural network architecture. Some of them differ in the number of layers and some in the configuration of the layers.\n",
    "\n",
    "In this blog post, we are going to focus on the VGG11 deep learning model. It is the simplest of all the configurations. It has 11 weight layers in total, so the name VGG11. 8 of them are convolutional layers, and 3 are fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4629e8",
   "metadata": {
    "papermill": {
     "duration": 0.00365,
     "end_time": "2023-10-13T12:02:09.463723",
     "exception": false,
     "start_time": "2023-10-13T12:02:09.460073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![VGG](https://debuggercafe.com/wp-content/uploads/2021/04/vgg_architectures_table.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3bcc2",
   "metadata": {
    "papermill": {
     "duration": 0.004168,
     "end_time": "2023-10-13T12:02:09.471752",
     "exception": false,
     "start_time": "2023-10-13T12:02:09.467584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "above figure shows all the network configurations of the VGG neural networks. Our focus will be on the VGG11 model (configuration A). The main reason being, it is the easiest to implement and will form the basis for other configurations and training for other VGG models as well.\n",
    "\n",
    "We can also see that VGG11 has 133 million parameters. Actually, the number is 132,863,336 to be exact. We will compare the number of parameters of our implemented model with this number to ensure that our implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba6300a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T12:02:09.483542Z",
     "iopub.status.busy": "2023-10-13T12:02:09.482844Z",
     "iopub.status.idle": "2023-10-13T12:02:09.502131Z",
     "shell.execute_reply": "2023-10-13T12:02:09.501033Z"
    },
    "papermill": {
     "duration": 0.028941,
     "end_time": "2023-10-13T12:02:09.504835",
     "exception": false,
     "start_time": "2023-10-13T12:02:09.475894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG11(nn.Module):\n",
    "    def __init__(self,ngpu, num_classes=10):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5feb2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T12:02:09.516499Z",
     "iopub.status.busy": "2023-10-13T12:02:09.516176Z",
     "iopub.status.idle": "2023-10-13T12:02:15.629967Z",
     "shell.execute_reply": "2023-10-13T12:02:15.628778Z"
    },
    "papermill": {
     "duration": 6.123082,
     "end_time": "2023-10-13T12:02:15.632698",
     "exception": false,
     "start_time": "2023-10-13T12:02:09.509616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "num_epochs = 25\n",
    "batch_size = 16\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = VGG11(ngpu,num_classes).to(device)\n",
    "# Handle multi-GPU if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    model = nn.DataParallel(model, list(range(ngpu)))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f0322d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T12:02:15.643035Z",
     "iopub.status.busy": "2023-10-13T12:02:15.642680Z",
     "iopub.status.idle": "2023-10-13T14:13:29.940481Z",
     "shell.execute_reply": "2023-10-13T14:13:29.939419Z"
    },
    "papermill": {
     "duration": 7874.305283,
     "end_time": "2023-10-13T14:13:29.942431",
     "exception": false,
     "start_time": "2023-10-13T12:02:15.637148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [704/704], Loss: 4.0232\n",
      "Accuracy of the network on the 5000 validation images: 17.82 %\n",
      "Epoch [2/25], Step [704/704], Loss: 2.9780\n",
      "Accuracy of the network on the 5000 validation images: 26.9 %\n",
      "Epoch [3/25], Step [704/704], Loss: 1.9974\n",
      "Accuracy of the network on the 5000 validation images: 35.12 %\n",
      "Epoch [4/25], Step [704/704], Loss: 2.7850\n",
      "Accuracy of the network on the 5000 validation images: 39.76 %\n",
      "Epoch [5/25], Step [704/704], Loss: 2.3832\n",
      "Accuracy of the network on the 5000 validation images: 40.94 %\n",
      "Epoch [6/25], Step [704/704], Loss: 2.4097\n",
      "Accuracy of the network on the 5000 validation images: 44.42 %\n",
      "Epoch [7/25], Step [704/704], Loss: 2.0495\n",
      "Accuracy of the network on the 5000 validation images: 47.66 %\n",
      "Epoch [8/25], Step [704/704], Loss: 1.6065\n",
      "Accuracy of the network on the 5000 validation images: 49.22 %\n",
      "Epoch [9/25], Step [704/704], Loss: 2.6602\n",
      "Accuracy of the network on the 5000 validation images: 49.54 %\n",
      "Epoch [10/25], Step [704/704], Loss: 2.5084\n",
      "Accuracy of the network on the 5000 validation images: 51.1 %\n",
      "Epoch [11/25], Step [704/704], Loss: 2.3638\n",
      "Accuracy of the network on the 5000 validation images: 53.02 %\n",
      "Epoch [12/25], Step [704/704], Loss: 1.7264\n",
      "Accuracy of the network on the 5000 validation images: 52.8 %\n",
      "Epoch [13/25], Step [704/704], Loss: 0.6246\n",
      "Accuracy of the network on the 5000 validation images: 53.02 %\n",
      "Epoch [14/25], Step [704/704], Loss: 1.6476\n",
      "Accuracy of the network on the 5000 validation images: 52.96 %\n",
      "Epoch [15/25], Step [704/704], Loss: 1.8301\n",
      "Accuracy of the network on the 5000 validation images: 53.94 %\n",
      "Epoch [16/25], Step [704/704], Loss: 0.9008\n",
      "Accuracy of the network on the 5000 validation images: 53.36 %\n",
      "Epoch [17/25], Step [704/704], Loss: 1.4624\n",
      "Accuracy of the network on the 5000 validation images: 55.26 %\n",
      "Epoch [18/25], Step [704/704], Loss: 1.1770\n",
      "Accuracy of the network on the 5000 validation images: 55.1 %\n",
      "Epoch [19/25], Step [704/704], Loss: 1.7868\n",
      "Accuracy of the network on the 5000 validation images: 54.7 %\n",
      "Epoch [20/25], Step [704/704], Loss: 1.1816\n",
      "Accuracy of the network on the 5000 validation images: 55.92 %\n",
      "Epoch [21/25], Step [704/704], Loss: 0.6667\n",
      "Accuracy of the network on the 5000 validation images: 55.06 %\n",
      "Epoch [22/25], Step [704/704], Loss: 0.6417\n",
      "Accuracy of the network on the 5000 validation images: 55.0 %\n",
      "Epoch [23/25], Step [704/704], Loss: 2.0715\n",
      "Accuracy of the network on the 5000 validation images: 53.7 %\n",
      "Epoch [24/25], Step [704/704], Loss: 1.4388\n",
      "Accuracy of the network on the 5000 validation images: 54.72 %\n",
      "Epoch [25/25], Step [704/704], Loss: 1.8007\n",
      "Accuracy of the network on the 5000 validation images: 54.24 %\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1c6442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:13:29.956671Z",
     "iopub.status.busy": "2023-10-13T14:13:29.956395Z",
     "iopub.status.idle": "2023-10-13T14:14:12.538187Z",
     "shell.execute_reply": "2023-10-13T14:14:12.537197Z"
    },
    "papermill": {
     "duration": 42.597718,
     "end_time": "2023-10-13T14:14:12.546847",
     "exception": false,
     "start_time": "2023-10-13T14:13:29.949129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 54.8 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7954.357246,
   "end_time": "2023-10-13T14:14:16.640219",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-13T12:01:42.282973",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
